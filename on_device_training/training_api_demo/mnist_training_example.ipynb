{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2022, Microsoft.\n",
    "\n",
    "## In this Notebook we'll build a simple convolutional neural network in ORT and train it to recognize handwritten digits using the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is ONNX Runtime Training APIS.\n",
    "ONNX Runtime Training apis gives you the ability to run an end to end training loops using just onnxruntime, you will still need to generate the training , eval and optimizer graphs using pyTorch first, but after that everything depends on onnxruntime only.\n",
    "\n",
    "These apis were introduced mainly for on device training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries\n",
    "\n",
    "Make sure to install onnxruntime-training's nightly version.\n",
    "\n",
    "```pip3 install onnxruntime-training -f https://download.onnxruntime.ai/onnxruntime_nightly_cu116.html```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bert_ort/adamlouly/wpy39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/bert_ort/adamlouly/wpy39/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_validation.py:114: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info.\n",
      "  warnings.warn(\"WARNING: failed to get cudart_version from onnxruntime build info.\")\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime.training.onnxblock as onnxblock\n",
    "from onnxruntime.training.api import CheckpointState, Module, Optimizer\n",
    "from onnxruntime import InferenceSession\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import io\n",
    "import netron\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs Generation:\n",
    "To run your training loop, first you need to generate training, eval (optional) and optimizer graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch class that we will use to generate the graphs.\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNet, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, model_input):\n",
    "        out = self.fc1(model_input)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Create a Simple instance.\n",
    "device = \"cpu\"\n",
    "batch_size, input_size, hidden_size, output_size = 64, 784, 500, 10\n",
    "pt_model = SimpleNet(input_size, hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating forward only graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random input.\n",
    "model_inputs = (torch.randn(batch_size, input_size, device=device),)\n",
    "\n",
    "model_outputs = pt_model(*model_inputs)\n",
    "if isinstance(model_outputs, torch.Tensor):\n",
    "    model_outputs = [model_outputs]\n",
    "    \n",
    "dynamic_axes = {}\n",
    "input_names = []\n",
    "output_names = []\n",
    "\n",
    "for i, model_input in enumerate(model_inputs):\n",
    "    input_name = f\"input-{i}\"\n",
    "    input_names.append(input_name)\n",
    "    dynamic_axes[input_name] = {}\n",
    "    for dim_idx in range(len(model_input.shape)):\n",
    "        dynamic_axes[input_name].update({dim_idx: f\"{input_name}_dim{dim_idx}\"})\n",
    "\n",
    "for i, model_output in enumerate(model_outputs):\n",
    "    output_name = f\"output-{i}\"\n",
    "    output_names.append(output_name)\n",
    "    dynamic_axes[output_name] = {}\n",
    "    for dim_idx in range(len(model_output.shape)):\n",
    "        dynamic_axes[output_name].update({dim_idx: f\"{output_name}_dim{dim_idx}\"})\n",
    "\n",
    "f = io.BytesIO()\n",
    "torch.onnx.export(\n",
    "    pt_model,\n",
    "    model_inputs,\n",
    "    f,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=False,\n",
    "    training=torch.onnx.TrainingMode.TRAINING,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    export_params=True,\n",
    "    keep_initializers_as_inputs=False,\n",
    ")\n",
    "onnx_model = onnx.load_model_from_string(f.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After creating forward only graph, we can now create the training graph.\n",
    "\n",
    "The first step is creating a simple class that inherits from onnxblock.TrainingModel, and define the loss function.\n",
    "the build function defines the output of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class with a Loss function.\n",
    "class SimpleModelWithCrossEntropyLoss(onnxblock.TrainingModel):\n",
    "    def __init__(self):\n",
    "        super(SimpleModelWithCrossEntropyLoss, self).__init__()\n",
    "        self.loss = onnxblock.loss.CrossEntropyLoss()\n",
    "\n",
    "    def build(self, output_name):\n",
    "        return self.loss(output_name), output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 20:32:20.394830072 [I:onnxruntime:Default, reshape_fusion.cc:42 ApplyImpl] Total fused reshape node count: 0\n",
      "2022-11-09 20:32:20.394877470 [I:onnxruntime:Default, concat_slice_elimination.cc:36 ApplyImpl] Total fused concat node count: 0\n"
     ]
    }
   ],
   "source": [
    "# Build the onnx model with loss\n",
    "simple_model = SimpleModelWithCrossEntropyLoss()\n",
    "\n",
    "# Building training graph and eval graph.\n",
    "with onnxblock.onnx_model(onnx_model) as accessor:\n",
    "    _ = simple_model(onnx_model.graph.output[0].name)\n",
    "    eval_model = accessor.eval_model\n",
    "\n",
    "# Building the optimizer graph\n",
    "optimizer = onnxblock.optim.AdamW()\n",
    "with onnxblock.onnx_model() as accessor:\n",
    "    _ = optimizer(simple_model.parameters())\n",
    "    optimizer_model = accessor.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 20:32:21.143151087 [W:onnxruntime:Default, checkpoint.cc:187 OrtSaveInternal] Checkpoint directory exists - data may be overwritten.\n"
     ]
    }
   ],
   "source": [
    "# Saving all the files to use them later for the training.\n",
    "\n",
    "trainable_params, non_trainable_params = simple_model.parameters()\n",
    "onnxblock.save_checkpoint((trainable_params, non_trainable_params), \"data/checkpoint.ckpt\")\n",
    "onnx.save(onnx_model, \"data/training_model.onnx\")\n",
    "onnx.save(optimizer_model, \"data/optimizer.onnx\")\n",
    "onnx.save(eval_model, \"data/eval_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can use netron to visualize the graphs.\n",
    "This is an example of how an eval graph looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'data/eval_model.onnx' at http://localhost:8081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8081)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netron.start(\"data/eval_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/C7W7cw2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After generating the required files for training, let's start training out model.\n",
    "##### Let's load the datasets from torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_batch_size = 1000\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                    transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the saved files to initialize the state, model and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint state.\n",
    "state = CheckpointState(\"data/checkpoint.ckpt\")\n",
    "\n",
    "# Create module.\n",
    "model = Module(\"data/training_model.onnx\", state, \"data/eval_model.onnx\")\n",
    "\n",
    "# Create optimizer.\n",
    "optimizer = Optimizer(\"data/optimizer.onnx\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing Loops definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to convert logits to predictions.\n",
    "def get_pred(logits):\n",
    "    return np.argmax(logits, axis=1)\n",
    "\n",
    "# Training Loop :\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        forward_inputs = [data.reshape(len(data),784).numpy(),target.numpy().astype(np.int32)]\n",
    "        train_loss, _ = model(forward_inputs)\n",
    "        optimizer.step()\n",
    "        model.reset_grad()\n",
    "        losses.append(train_loss)\n",
    "\n",
    "    print('Epoch: {},Train Loss: {:.4f}'.format(epoch+1, sum(losses)/len(losses)))\n",
    "\n",
    "# Test Loop :\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    metric = evaluate.load('accuracy')\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        forward_inputs = [data.reshape(len(data),784).numpy(),target.numpy().astype(np.int32)]\n",
    "        test_loss, logits = model(forward_inputs)\n",
    "        metric.add_batch(references=target, predictions=get_pred(logits))\n",
    "        losses.append(test_loss)\n",
    "\n",
    "    metrics = metric.compute()\n",
    "    \n",
    "    print('Epoch: {}, Test Loss: {:.4f}, Accuracy : {:.2f}'.format(epoch+1, sum(losses)/len(losses), metrics['accuracy']*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1,Train Loss: 0.2199\n",
      "Epoch: 1, Test Loss: 0.1269, Accuracy : 95.92\n",
      "Epoch: 2,Train Loss: 0.0901\n",
      "Epoch: 2, Test Loss: 0.0859, Accuracy : 97.03\n",
      "Epoch: 3,Train Loss: 0.0565\n",
      "Epoch: 3, Test Loss: 0.0596, Accuracy : 97.92\n",
      "Epoch: 4,Train Loss: 0.0372\n",
      "Epoch: 4, Test Loss: 0.0373, Accuracy : 98.75\n",
      "Epoch: 5,Train Loss: 0.0281\n",
      "Epoch: 5, Test Loss: 0.0539, Accuracy : 98.29\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Inference model and Run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export_model_for_inferencing(\"data/inference_model.onnx\",[\"output-0\"])\n",
    "session = InferenceSession('data/inference_model.onnx',providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label :  [7]\n"
     ]
    }
   ],
   "source": [
    "# getting one example from test list to try inference.\n",
    "data = next(iter(test_loader))[0][0].reshape(1,784).numpy()\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name \n",
    "output = session.run([output_name], {input_name: data})\n",
    "\n",
    "print(\"Predicted Label : \",get_pred(output[0]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "672f1f04373ba0bfe81ddf50d232d84b9b6dccc610a78c59e8ffefd8db4edeac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
