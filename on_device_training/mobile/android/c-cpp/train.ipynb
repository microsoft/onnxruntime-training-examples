{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning on the edge: Offline Step - Training validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will validate the concept of transfer learning with an example dataset, and evaluate the ort training api.\n",
    "\n",
    "We will use an animal dataset and learn to classify animals among 4 categories: `Dog`, `Cat`, `Elephant` and `Cow`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we begin by loading in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# load the dataset files into a dictionary\n",
    "def load_dataset_files():\n",
    "    animals = {\n",
    "        \"dog\": [],\n",
    "        \"cat\": [],\n",
    "        \"elephant\": [],\n",
    "        \"cow\": []\n",
    "    }\n",
    "\n",
    "    for animal in animals:\n",
    "        animals[animal] = glob.glob(\n",
    "            f\"data/images/{animal}/*\")\n",
    "\n",
    "    return animals\n",
    "\n",
    "animals = load_dataset_files()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a way to process the input image and make it ready to be processed by the onnxruntime training api.\n",
    "\n",
    "This is done by:\n",
    "  - Loading the image\n",
    "  - Cropping along the longer dimension to get a square image\n",
    "  - Resizing the image to be of shape [3 x 224 x 224]\n",
    "  - Normalizing the tensor by subtracting the mean (0.485, 0.456, 0.406) and dividing by the standard deviation (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Preprocess the images and convert to tensors as expected by the model\n",
    "# Makes the image a square and resizes it to 224x224 as is expected by\n",
    "# the mobilenetv2 model\n",
    "# Normalize the image by subtracting the mean (0.485, 0.456, 0.406) and\n",
    "# dividing by the standard deviation (0.229, 0.224, 0.225)\n",
    "def image_file_to_tensor(file):\n",
    "    from PIL import Image\n",
    "\n",
    "    image = Image.open(file)\n",
    "    width, height = image.size\n",
    "    if width > height:\n",
    "        left = (width - height) // 2\n",
    "        right = (width + height) // 2\n",
    "        top = 0\n",
    "        bottom = height\n",
    "    else:\n",
    "        left = 0\n",
    "        right = width\n",
    "        top = (height - width) // 2\n",
    "        bottom = (height + width) // 2\n",
    "    image = image.crop((left, top, right, bottom)).resize((224, 224))\n",
    "\n",
    "    pix = np.transpose(np.array(image, dtype=np.float32), (2, 0, 1))\n",
    "    pix = pix / 255.0\n",
    "    pix[0] = (pix[0] - 0.485) / 0.229\n",
    "    pix[1] = (pix[1] - 0.456) / 0.224\n",
    "    pix[2] = (pix[2] - 0.406) / 0.225\n",
    "    return pix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block defines some training metadata variables and establishes the number of training samples and number of training epochs.\n",
    "\n",
    "For this demo, we will pick the number of training samples as 20 per class (20 dogs, 20 cats, 20 elephants and 20 cows), and the number of epochs as 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metadata\n",
    "dog, cat, elephant, cow = \"dog\", \"cat\", \"elephant\", \"cow\" # labels\n",
    "label_to_id_map = {\n",
    "    \"dog\": 0,\n",
    "    \"cat\": 1,\n",
    "    \"elephant\": 2,\n",
    "    \"cow\": 3\n",
    "} # label to index mapping\n",
    "\n",
    "num_samples_per_class = 20\n",
    "num_epochs = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define our training loop. This is where we get to interact with the ort training api.\n",
    "\n",
    "In particular, we instantiate 3 variables that are very tightly coupled:\n",
    "1. The checkpoint state - contains the state of the model parameters at any given time.\n",
    "2. The training module - responsible for executing the training and eval graphs:\n",
    "   - Executing the training graph results in the computation of the training loss, and the gradients associated with the model parameters.\n",
    "   - Executing the eval graph results in the computation of the eval loss.\n",
    "   - Switching between the train and eval mode is done by calling `module.train()` or `module.eval()`.\n",
    "3. The optimizer - responsible for updating the model parameters in the direction of their computed gradients.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime.training.api as orttraining\n",
    "\n",
    "# Instantiate the training session by defining the checkpoint state, module, and optimizer\n",
    "# The checkpoint state contains the state of the model parameters at any given time.\n",
    "checkpoint_state = orttraining.CheckpointState(\n",
    "    \"training_artifacts/mobilenetv2.ckpt\")\n",
    "\n",
    "model = orttraining.Module(\n",
    "    \"training_artifacts/mobilenetv2_training.onnx\",\n",
    "    checkpoint_state,\n",
    "    \"training_artifacts/mobilenetv2_eval.onnx\",\n",
    ")\n",
    "\n",
    "optimizer = orttraining.Optimizer(\n",
    "    \"training_artifacts/mobilenetv2_optimizer.onnx\", model\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for index in range(num_samples_per_class):\n",
    "        batch = []\n",
    "        labels = []\n",
    "        for animal in animals:\n",
    "            batch.append(image_file_to_tensor(animals[animal][index]))\n",
    "            labels.append(label_to_id_map[animal])\n",
    "        batch = np.stack(batch)\n",
    "        labels = np.array(labels, dtype=np.int32)\n",
    "\n",
    "        # ort training api - training model execution outputs the training loss and the parameter gradients\n",
    "        loss += model([batch, labels])[0]\n",
    "        # ort training api - update the model parameters by taking a step in the direction of the gradients\n",
    "        optimizer.step()\n",
    "        # ort training api - reset the gradients to zero so that new gradients can be computed in the next run\n",
    "        model.lazy_reset_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss {loss/num_samples_per_class}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inferencing on unseen data to verify the training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "from onnxruntime.capi import _pybind_state as C\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ort training api - export the model for so that it can be used for inferencing\n",
    "model.export_model_for_inferencing(\"inference_artifacts/inference.onnx\", [\"output\"])\n",
    "\n",
    "# Run inference on the exported model\n",
    "session = InferenceSession(\"inference_artifacts/inference.onnx\", providers=C.get_available_providers())\n",
    "\n",
    "def softmax(logits):\n",
    "    return (np.exp(logits)/np.exp(logits).sum())\n",
    "\n",
    "def predict(test_file, test_name):\n",
    "    logits = session.run([\"output\"], {\"input\": np.stack([image_file_to_tensor(test_file)])})\n",
    "    probabilities = softmax(logits) * 100\n",
    "    display(Image(filename=test_file))\n",
    "    print_prediction(probabilities, test_name)\n",
    "\n",
    "def print_prediction(prediction, test_name):\n",
    "    print(f\"test\\t{dog}\\t{cat}\\t{elephant}\\t{cow}\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"{test_name}\\t{prediction[0][0][0]:.2f}\\t{prediction[0][0][1]:.2f}\\t{prediction[0][0][2]:.2f}\\t\\t{prediction[0][0][3]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample image (test1.jpg)\n",
    "predict(\"inference_artifacts/test1.jpg\", \"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on another sample image (test2.jpg)\n",
    "predict(\"inference_artifacts/test2.jpg\", \"test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
