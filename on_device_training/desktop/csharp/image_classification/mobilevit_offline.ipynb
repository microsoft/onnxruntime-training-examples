{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileViT Offline Processing\n",
    "\n",
    "On-device training requires steps that happen off the device, referred to as \"offline\" steps. \n",
    "\n",
    "This notebook contains the offline processing steps for an example using MobileViT for facial expression recognition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training artifact generation\n",
    "\n",
    "In order to train on the device, the following files are required: a model checkpoint with the weights, the training model ONNX file, the optimizer ONNX file, and the evaluation model ONNX file. \n",
    "\n",
    "The generate_artifacts method simplifies this process, allowing you to pass in an initial ONNX model (for example, imported from HuggingFace Transformers), specify a loss type and optimizer (both required), and will generate the required training artifacts for you.\n",
    "\n",
    "Before passing to the generate_artifacts function, the model is configured to suit the dataset: for example, the random input that the model is built off of has the same image dimensions as the dataset images, and the number of labels is configured to reflect the dataset. \n",
    "\n",
    "Although this example uses PyTorch to export a HuggingFace Transformers model into an ONNX file to be passed to generate_artifacts, any method of creating or exporting an ONNX file can be used with generate_artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import onnx\n",
    "from onnxruntime.training import artifacts\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the configuration to reflect the number of labels used in the dataset\n",
    "config = transformers.MobileViTConfig.from_pretrained(\"apple/mobilevit-xx-small\", num_labels=7)\n",
    "model = transformers.MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-xx-small\", config=config, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the random input generated should follow the shape / dimensions of the images you will train on. The training artifact generation step will fail if the width and height of the random input are not powers of 2. However you choose to process your images, they should be resized to have the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_name = \"mobilevit.onnx\"\n",
    "\n",
    "# generates random pixel values for 5 images\n",
    "random_input = {\"pixel_values\": torch.rand(5, 3, 256, 256)}\n",
    "\n",
    "torch.onnx.export(model, random_input, onnx_name,\n",
    "                    input_names=[\"pixel_values\"], output_names=[\"outputs\"],\n",
    "                    export_params=True,\n",
    "                    dynamic_axes={\n",
    "                        \"pixel_values\": {0: \"batch_size\"},\n",
    "                        \"outputs\": {0: \"batch_size\"},\n",
    "                    },\n",
    "                    do_constant_folding=False,\n",
    "                    training=torch.onnx.TrainingMode.TRAINING,\n",
    "                    opset_version=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we only want to fine-tune this example, only the \"classifier.weight\" and \"classifier.bias\" parameters are passed in as parameters that requires_grad (requires gradient / trainable). In your example, it might be necessary to look at the exported ONNX model from the previous step in a program such as Netron in order to identify the names of the parameters that you want to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad = []\n",
    "frozen_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"classifier.weight\" or name == \"classifier.bias\":\n",
    "        requires_grad.append(name)\n",
    "    else:\n",
    "        frozen_params.append(name)\n",
    "\n",
    "for name, param in model.named_buffers():\n",
    "    frozen_params.append(name)\n",
    "\n",
    "\n",
    "onnx_model = onnx.load(onnx_name)\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    loss=artifacts.LossType.CrossEntropyLoss,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    do_constant_folding=False,\n",
    "    training=torch.onnx.TrainingMode.TRAINING\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
