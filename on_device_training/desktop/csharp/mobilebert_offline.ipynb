{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, all the necessary offline steps are executed for MobileBERT for a masked language modeling task. The necessary training artifacts are generated, as well as the processed data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import onnx\n",
    "import onnxruntime.training.onnxblock as onnxblock\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from transformers import MobileBertConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating artifacts\n",
    "\n",
    "This section generates the necessary training artifacts: the training version of the ONNX model, the evaluation version of the ONNX model, and the optimizer.\n",
    "\n",
    "These are exported as ONNX files and later imported in the C# app using the C# ONNX Runtime Training API.\n",
    "\n",
    "In order to generate the training ONNX model, a loss node needs to be added onto the model. But MobileBERT for Masked LM will also calculate the losses if labels are provided, so the FlatModel is a work-around to add labels to the input in the forward pass of the model. This is then exported, and ORT generate_artifacts is used to generate the training artifacts.\n",
    "\n",
    "The ORT generate_artifacts method must be passed a model with a loss node & the original torch model is referenced to determine what model parameters should be frozen + which model parameters should have the requires_grad option toggled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MobileBertConfig(num_hidden_layers=2)\n",
    "model = transformers.MobileBertForMaskedLM.from_pretrained('google/mobilebert-uncased', config=config)\n",
    "# model = transformers.AutoModel.from_pretrained('google/mobilebert-uncased')\n",
    "model_name = 'mobilebert-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *local_inputs):\n",
    "        return self.model(inputs.input_ids, inputs.attention_mask, inputs.token_type_ids, labels=labels)\n",
    "\n",
    "model = FlatModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    (inputs[\"input_ids\"], \n",
    "      inputs[\"attention_mask\"],\n",
    "      inputs[\"token_type_ids\"],\n",
    "      labels),\n",
    "    f\"training_artifacts/{model_name}.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"],\n",
    "    output_names=[\"loss\", \"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"token_type_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"labels\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"logits \": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "    },\n",
    "    export_params=True,\n",
    "    do_constant_folding=False,\n",
    "    training=torch.onnx.TrainingMode.TRAINING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.training import artifacts\n",
    "import onnx\n",
    "\n",
    "requires_grad = []\n",
    "frozen_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        requires_grad.append(name)\n",
    "    else:\n",
    "        frozen_params.append(name)\n",
    "\n",
    "for name, param in model.named_buffers():\n",
    "    frozen_params.append(name)\n",
    "\n",
    "model = onnx.load(f\"training_artifacts/{model_name}.onnx\")\n",
    "\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    model,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    artifact_directory=\"training_artifacts/\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating tokens\n",
    "\n",
    "This section tokenizes the dataset and then writes it into JSON files.\n",
    "\n",
    "Since a C# tokenizer for this task doesn't exist yet in ORT extensions, this step is done offline, and the data is then loaded into the C# program.\n",
    "\n",
    "Since the model requires a masked input + a \"label\" (a corresponding unmasked input), these are artificially generated by randomly masking a word in a sequence and keeping the original sequence as the label. \n",
    "\n",
    "A JSON format was chosen because it can easily be parsed with C# libraries, but you may use any data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, pad_to_len):\n",
    "    \"\"\"\n",
    "    Takes in a Dataset with a \"text\" feature, as well as an int for what to pad the sequences to.\n",
    "\n",
    "    The sequences are both padded and truncated so they are all the same length.\n",
    "\n",
    "    Outputs a Dataset with the following features: text, input_ids, token_type_ids, attention_mask, labels\n",
    "    \"\"\"\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
    "    # filter out empty strings to remove unnecessary processing\n",
    "    examples[\"text\"] = [sent for sent in examples[\"text\"] if len(sent) > 0]\n",
    "    labels = tokenizer(examples[\"text\"], padding=\"max_length\", max_length=pad_to_len, truncation=True, return_tensors=\"pt\")\n",
    "    masked_examples = [mask(sent, pad_to_len) for sent in examples[\"text\"]]\n",
    "    inputs = tokenizer(masked_examples, padding=\"max_length\", max_length=pad_to_len, truncation=True, return_tensors=\"pt\")\n",
    "    labels = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id, labels[\"input_ids\"], -100)\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "def mask(sent, pad_to_len):\n",
    "    \"\"\" \n",
    "    Randomly replaces a word in the sentence with \"[MASK]\", ignoring punctuation\n",
    "    \"\"\"\n",
    "    sent_words = sent.split()\n",
    "    mask_index = random.randint(0, min(len(sent_words), pad_to_len) - 1)\n",
    "    # replace random index with mask word, leaving punctuation as is\n",
    "    # ... this preprocessing means that the token masked might be the <unk> word\n",
    "    masked_words = [sent_words[ind] if ind != mask_index else re.sub(\"[a-zA-Z']+\", \"[MASK]\", sent_words[ind]) for ind in range(len(sent_words))]\n",
    "    return ' '.join(masked_words)\n",
    "\n",
    "def generate_tokens(corpus):\n",
    "    \"\"\"\n",
    "    Takes in a Dataset with a \"text\" feature.\n",
    "\n",
    "    Returns a Dataset with the following features: text, input_ids, token_type_ids, attention_mask, special_tokens_mask\n",
    "    \"\"\"\n",
    "    # pad_to_len must be calculated before the batching happens to create consistent sizes in the resulting tensor\n",
    "    # pad_to_len = max([len(sent) for sent in corpus[\"text\"]])\n",
    "    pad_to_len = 80 # shortened for demonstration purposes\n",
    "    return corpus.map(tokenize_function, batched=True, fn_kwargs={\"pad_to_len\": pad_to_len})\n",
    "\n",
    "def generate_json_dict(token_dataset):\n",
    "    \"\"\"\n",
    "    Takes in a Dataset with the following features: text, input_ids, token_type_ids, attention_mask, labels\n",
    "\n",
    "    Basically changes the 2d Python lists into two fields: a shape & a flattened list, for easier conversion to OnnxValues\n",
    "\n",
    "    Returns a dictionary with the following keys: input_ids, input_size, token_type_ids, token_type_size, attention_mask, attention_mask_size, special_tokens_mask, special_tokens_size\n",
    "    \"\"\"\n",
    "    json_dict = {}\n",
    "    keys_to_convert = [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "    for key_name in keys_to_convert:\n",
    "        # add field for the shape of the tensor\n",
    "        json_dict[key_name + \"_shape\"] = [len(token_dataset[key_name]), len(token_dataset[key_name][0])]\n",
    "        # flatten list\n",
    "        json_dict[key_name] = [num for sent in token_dataset[key_name] for num in sent]\n",
    "    \n",
    "    return json_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"wikitext\" \n",
    "dataset_config = \"wikitext-2-v1\"\n",
    "# corpus = type DatasetDict with three Datasets: test, train, validation\n",
    "corpus = load_dataset(dataset_name, dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens_dataset = generate_tokens(corpus[\"test\"])\n",
    "test_tokens = generate_json_dict(test_tokens_dataset)\n",
    "train_tokens_dataset = generate_tokens(corpus[\"train\"])\n",
    "train_tokens = generate_json_dict(train_tokens_dataset)\n",
    "validation_tokens_dataset = generate_tokens(corpus[\"validation\"])\n",
    "validation_tokens = generate_json_dict(validation_tokens_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all the tokens to a json file\n",
    "file_names = [\"test_tokens.json\", \"train_tokens.json\", \"validation_tokens.json\"]\n",
    "token_dicts = [test_tokens, train_tokens, validation_tokens]\n",
    "\n",
    "def write_dicts_to_files(file_names, dicts):\n",
    "    # assumes file_names and dicts are 2 lists w/ the same lengths\n",
    "    for i in range(len(file_names)):\n",
    "        with open(file_names[i], \"w\") as json_file:\n",
    "            json.dump(dicts[i], json_file)\n",
    "\n",
    "write_dicts_to_files(file_names, token_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
