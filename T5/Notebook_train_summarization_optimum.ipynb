{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine Tuning T5 model with Azure ML using Azure Container for PyTorch \n",
        "\n",
        "This tutorial shows how to fine tune the T5 model to generate a summary of a news article. We then deploy it to an online endpoint for real time inference. The model is trained on a tiny sample of the dataset with a small number of epochs to illustrate the fine tuning approach.\n",
        "\n",
        "### Learning Objectives\n",
        "- Fine tune the T5 small model for the `Summarization` task with `Azure ML` \n",
        "- Leverage the `ACPT` environment with state of art accelerators\n",
        "- Increase training efficiency using [`DeepSpeed`](https://github.com/microsoft/DeepSpeed) and [`ONNX Runtime`](https://github.com/microsoft/onnxruntime)\n",
        "- Model Evaluation uring prebuilt component\n",
        "- Register the model with AzureML\n",
        "- Deploy and inference using MIR and ONNX Runtime\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "###### T5-small is a 60 million parameter model based on text-to-text framework and is used for several NLP tasks, including machine translation, document summarization, question answering pretrained on Colossal Clean Crawled Corpus (C4) dataset.\n",
        "\n",
        "translation (green), linguistic acceptability (red), sentence similarity (yellow), and document summarization (blue)\n",
        "\n",
        "##### In this workshop, we will be fine tuning the document summarization task.\n",
        "\n",
        "\n",
        "![Image](assets/t5modelcard.PNG)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Prerequisites to install Azure ML Python SDK Version 2 \n",
        "Please restart kernel after pip installs to sync environment with new modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install azure-ai-ml azure-identity datasets azure-cli\n",
        "%pip install onnxruntime==1.15.1 transformers==4.29.2 torch==2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !az login --use-device-code"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 2. Connect to Azure Machine Learning workspace\n",
        "\n",
        "Before we dive in the code, you'll need to connect to your workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
        "\n",
        "For this lab, we've already setup an AzureML Workspace for you. If you'd like to learn more about `Workspace`s, please reference [`AzureML's documentation`](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?view=azureml-api-2&tabs=azure-portal).\n",
        "\n",
        "We are using the `DefaultAzureCredential` to get access to workspace. `DefaultAzureCredential` should be capable of handling most scenarios. If you want to learn more about other available credentials, go to [`Set up authentication`](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk&view=azureml-api-2) for more available credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684438680625
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import AzureCliCredential\n",
        "\n",
        "credential = AzureCliCredential()\n",
        "credential.get_token(\"https://management.azure.com/.default\")\n",
        "\n",
        "ml_client = MLClient(\n",
        "    credential,\n",
        "    subscription_id=\"ed2cab61-14cc-4fb3-ac23-d72609214cfd\",\n",
        "    resource_group_name=\"AMLDataCache\",\n",
        "    workspace_name=\"datacachetest\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 3. Create a compute\n",
        "\n",
        "Azure Machine Learning needs a compute resource to run a job. This resource can be single or multi-node machines with Linux or Windows OS. In the following example script, we provision a `Standard_ND40rs_v2` SKU which is infiniband enabled to provide higher node communication bandwidth and low latency with mellanox drivers to create an Azure Machine Learning compute. You can get the list and more detail [here](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-hpc#rdma-capable-instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684438712706
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "experiment_name = \"T5-Summarization-news-summary\"\n",
        "\n",
        "# If you already have a gpu cluster, mention it here. Else will create a new one\n",
        "compute_cluster = \"v100\"\n",
        "try:\n",
        "    compute = ml_client.compute.get(compute_cluster)\n",
        "    print(\"successfully fetched compute:\", compute.name)\n",
        "except Exception as ex:\n",
        "    print(\"failed to fetch compute:\", compute_cluster)\n",
        "    print(\"creating new Standard_ND40rs_v2 compute\")\n",
        "    compute = AmlCompute(\n",
        "        name=compute_cluster,\n",
        "        size=\"Standard_ND40rs_v2\", # Info on Standard_ND40rs_v2 SKU: https://learn.microsoft.com/en-us/azure/virtual-machines/ndv2-series\n",
        "        min_instances=1,\n",
        "        max_instances=2,  # For multi node training set this to an integer value more than 1\n",
        "    )\n",
        "    ml_client.compute.begin_create_or_update(compute).wait()\n",
        "    print(\"successfully created compute:\", compute.name)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 4. Create a job environment using Azure Container for Pytorch\n",
        "\n",
        "We will be creating a custom environment using existing ACPT curated environment consisting of state of art technologies like Deepspeed, OnnxRuntime. You can get more detail from [Custom Environment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-azure-container-for-pytorch-environment?view=azureml-api-2)\n",
        "\n",
        "\n",
        "view the [Environments in Azure Machine Learning studio](https://ml.azure.com/environments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684438754226
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Environment, BuildContext\n",
        "import datetime\n",
        "\n",
        "# Define an environment name\n",
        "env_name = \"env-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
        "\n",
        "env_docker_context = Environment(\n",
        "    build=BuildContext(path=\"src/Environment/context\"),\n",
        "    name=env_name,\n",
        "    description=\"Environment created from a Docker context.\",\n",
        ")\n",
        "ml_client.environments.create_or_update(env_docker_context)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Pick the dataset for fine-tuning the model\n",
        "\n",
        "The [CNN DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. It is larger than 1GB when uncompressed. \n",
        "\n",
        "We want this sample to run quickly, so a copy of the fraction of dataset is used for fine tuning job.This means the fine tuned model will have lower accuracy, hence it should not be put to real-world use. \n",
        "* Visualize some data rows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684438763102
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option(\n",
        "    \"display.max_colwidth\", 1000\n",
        ")\n",
        "\n",
        "train_df = pd.read_json(\"./src/Finetune/cnn_daily.jsonl\", lines=True)\n",
        "train_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 6. Finetune the T5 small model for Summarization task\n",
        "\n",
        "Leveraging Deepspeed and Onnxruntime accelarators for improving the efficiency for memory and compute and in turn reduce the training cost. \n",
        "\n",
        "The table below details some of the parameters passed to the training job.\n",
        "\n",
        "| Parameters/accelarators | Description |\n",
        "| ----------------- | --- |\n",
        "| model_name | The name of the model getting finetuned. Here we specify T5-small. |\n",
        "| ort | [Onnx runtime](https://github.com/microsoft/onnxruntime) accelarates 2x speed up in training time for SOTA models and optimizes memory to fit larger model such as GPT3 on 16GB GPU which would otherwise run out of mmemory. |\n",
        "| deepspeed | [Deepspeed](https://github.com/microsoft/deepspeed) enables running billions of parameter models distributed across GPUs and provide different stages for memory and compute efficiency. |\n",
        "| number of epochs | 1 |\n",
        "| max train samples | 10 |\n",
        "| Nebula | checkpointing |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684438864620
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "\n",
        "env_name = \"MSBuildLab110_env@latest\" # FOR DEMO\n",
        "\n",
        "job = command(\n",
        "    code=\"src/Finetune/\",\n",
        "    command=\"python train_summarization_optimum.py \\\n",
        "        --deepspeed ds_config.json \\\n",
        "        --model_name_or_path t5-small \\\n",
        "        --dataset_name cnn_dailymail \\\n",
        "        --max_train_samples=10 \\\n",
        "        --max_eval_samples=10 \\\n",
        "        --dataset_config '3.0.0' \\\n",
        "        --do_train \\\n",
        "        --num_train_epochs=1 \\\n",
        "        --per_device_train_batch_size=16 \\\n",
        "        --per_device_eval_batch_size=16  \\\n",
        "        --output_dir outputs \\\n",
        "        --overwrite_output_dir \\\n",
        "        --fp16 \\\n",
        "        --optim adamw_ort_fused\",\n",
        "    compute=compute_cluster,\n",
        "    environment=env_name,\n",
        "    instance_count=1,  \n",
        "    distribution={\n",
        "        \"type\": \"PyTorch\",\n",
        "        \"process_count_per_instance\": 8,\n",
        "    },\n",
        ")\n",
        "job = ml_client.jobs.create_or_update(job)\n",
        "job.studio_url"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results show **~300%** improvement of Fine-tune job with 100 epoch and CNN_Daily dataset with ORT, Deepspeed and Nebula checkpointing\n",
        "\n",
        "| Accelerator | Train Runtime | Train Samples Per Sec | Train Steps Per Sec | Train Loss | FLOPS          | AML Link                                                   |\n",
        "| ----------- | ------------- | --------------------- | ------------------- | ---------- | -------------- | ---------------------------------------------------------- |\n",
        "| -           | 1d 9h 6m 22s  | 241.782               | 0.945               | 1.704454   | 7.7719 x 10^18 | [100epoch_and_no_accelarator][100epoch_and_no_accelarator] |\n",
        "| ORT+DS      | 10h 53 m 36s  | 744.384               | 2.909               | 1.850196   | 7.8759 x 10^18 | [100epoch_with_DS_ORT_Nebula][100epoch_with_DS_ORT_Nebula] |\n",
        "\n",
        "[100epoch_and_no_accelarator]: https://ml.azure.com/experiments/id/236409a2-f1d9-41da-9b7e-19ec7bfd23a6/runs/tough_cord_3wz4fymfjf?wsid=/subscriptions/ed2cab61-14cc-4fb3-ac23-d72609214cfd/resourcegroups/AMLDataCache/workspaces/datacachetest&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
        "[100epoch_with_DS_ORT_Nebula]: https://ml.azure.com/experiments/id/236409a2-f1d9-41da-9b7e-19ec7bfd23a6/runs/tough_fennel_7n42y51slk?wsid=/subscriptions/ed2cab61-14cc-4fb3-ac23-d72609214cfd/resourcegroups/AMLDataCache/workspaces/datacachetest&tid=72f988bf-86f1-41af-91ab-2d7cd011db47"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 7. Operationalizing the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### 7.1 Register Onnx model\n",
        "**NOTE: STEP 6 (T5 FINETUNE) MUST COMPLETE BEFORE RUNNING THIS CELL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684439751829
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.entities import Model\n",
        "import time\n",
        "\n",
        "timestamp = str(int(time.time()))\n",
        "model_name = \"T5Model\"\n",
        "job.name = \"loyal_nose_p0r4vzq8w8\" # FOR DEMO\n",
        "\n",
        "#Onnx model registration\n",
        "modelpath = \"azureml://jobs/{jobname}/outputs/artifacts/outputs/onnx\".format(jobname = job.name)\n",
        "cloud_model = Model(\n",
        "    path=modelpath,\n",
        "    name=model_name+\"_onnx\",\n",
        "    type=AssetTypes.CUSTOM_MODEL,\n",
        "    description=\"Model created from cloud path.\",\n",
        "    version=timestamp,\n",
        ")\n",
        "ml_client.models.create_or_update(cloud_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### 7.2 Create online endpoint\n",
        "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684439142063
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
        "import datetime\n",
        "\n",
        "# Define an endpoint name\n",
        "endpoint_name = \"endpt-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name = endpoint_name, \n",
        "    description=\"this is a endpoint for T5 summarization model\",\n",
        "    auth_mode=\"key\"\n",
        ")\n",
        "\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).wait()\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### 7.3 Deploy scoring file to the endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684439501278
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import (\n",
        "    CodeConfiguration,\n",
        "    ManagedOnlineDeployment\n",
        ")\n",
        "\n",
        "model = ml_client.models.get(name=model_name+\"_onnx\", version = timestamp)\n",
        "\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    model=model,\n",
        "    environment=\"MSBuildLab110_env@latest\",\n",
        "    code_configuration=CodeConfiguration(\n",
        "        code=\".\", scoring_script=\"src/Operationalize/score_onnx.py\"\n",
        "    ),\n",
        "    instance_type=\"Standard_F8s_v2\",\n",
        "    instance_count=1,\n",
        ")\n",
        "\n",
        "ml_client.online_deployments.begin_create_or_update(blue_deployment)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### 7.4: Invoke the endpoint to score data by using your model\n",
        "**NOTE: STEP 7.3 (ENDPOINT DEPLOYMENT) MUST COMPLETE BEFORE RUNNING THIS CELL**\n",
        "\n",
        "Test the blue deployment with some sample data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "endpoint_name = \"MSBuildLab110-endpoint\" # FOR DEMO\n",
        "\n",
        "ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=endpoint_name,\n",
        "    deployment_name=\"blue\",\n",
        "    request_file=\"src/Operationalize/payload.json\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### 7.5: Delete the online endpoint\n",
        "Don't forget to delete the online endpoint, else you will leave the billing meter running for the compute used by the endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684306030654
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ml_client.online_endpoints.begin_delete(name=endpoint_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Aside: Scoring files for ONNX Runtime Inference vs. Hugging Face Inference\n",
        "\n",
        "![Image](assets/T5_beamsearch.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684906894100
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import json \n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "onnxruntime.set_default_logger_severity(3)\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Documentation: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints\n",
        "# Troubleshooting: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints\n",
        "  \n",
        "# The init() method is called once, when the web service starts up.\n",
        "def init():  \n",
        "    global SESS\n",
        "    global TOKENIZER\n",
        "    # The AZUREML_MODEL_DIR environment variable indicates  \n",
        "    # a directory containing the model file you registered.  \n",
        "    # model_filename = os.path.join(os.environ['AZUREML_MODEL_DIR'], \"onnx/outputs_beam_search.onnx\")  \n",
        "\n",
        "    model_filename = \"src/Model/onnx/outputs_beam_search.onnx\" \n",
        "    SESS = onnxruntime.InferenceSession(model_filename, providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "    TOKENIZER = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "  \n",
        "# The run() method is called each time a request is made to the scoring API.  \n",
        "def run(data):\n",
        "    json_data = json.loads(data)\n",
        "    input_data = json_data[\"inputs\"][\"article\"]\n",
        "    \n",
        "    input_ids = TOKENIZER(str(input_data), return_tensors=\"pt\").input_ids\n",
        "\n",
        "    ort_inputs = {\n",
        "        \"input_ids\": np.array(input_ids, dtype=np.int32),\n",
        "        \"max_length\": np.array([512], dtype=np.int32),\n",
        "        \"min_length\": np.array([0], dtype=np.int32),\n",
        "        \"num_beams\": np.array([1], dtype=np.int32),\n",
        "        \"num_return_sequences\": np.array([1], dtype=np.int32),\n",
        "        \"length_penalty\": np.array([1.0], dtype=np.float32),\n",
        "        \"repetition_penalty\": np.array([1.0], dtype=np.float32)\n",
        "    }\n",
        "    \n",
        "    out = SESS.run(None, ort_inputs)[0][0] # 0th batch, 0th sample\n",
        "\n",
        "    summary = TOKENIZER.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    # You can return any JSON-serializable object.\n",
        "    return {\"summary\": summary}\n",
        "\n",
        "def test():\n",
        "    # NOTE: You need to comment out model_filename = os.path.join(...) in init() for local testing\n",
        "    init()\n",
        "    payload = {\n",
        "        \"inputs\": {\n",
        "            \"article\": [\"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"],\n",
        "            \"params\": {\n",
        "                \"max_new_tokens\": 512\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    payload = str.encode(json.dumps(payload))\n",
        "    res = run(payload)\n",
        "    print(res)\n",
        "\n",
        "    # timed run\n",
        "    start = time.time()\n",
        "    for i in range(10):\n",
        "        _ = run(payload)\n",
        "    diff = time.time() - start\n",
        "    print(f\"time {diff/10} sec\")\n",
        "\n",
        "test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684442248813
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from transformers import pipeline\n",
        "import json \n",
        "import time\n",
        "import joblib\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Documentation: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints\n",
        "# Troubleshooting: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints\n",
        "  \n",
        "# The init() method is called once, when the web service starts up.\n",
        "def init():  \n",
        "    global MODEL\n",
        "    global TOKENIZER\n",
        "    # The AZUREML_MODEL_DIR environment variable indicates  \n",
        "    # a directory containing the model file you registered.  \n",
        "    # model_path = os.path.join(os.environ['AZUREML_MODEL_DIR'])\n",
        "    # model_file = os.path.join(os.environ['AZUREML_MODEL_DIR'], \"pytorch_model.bin\")\n",
        "\n",
        "    model_path = \"src/Model\"\n",
        "    model_file = \"src/Model/pytorch_model.bin\"\n",
        "    TOKENIZER = AutoTokenizer.from_pretrained(model_path)\n",
        "    config = AutoConfig.from_pretrained(model_path)\n",
        "    MODEL = AutoModelForSeq2SeqLM.from_pretrained(model_file, config=config) \n",
        "    \n",
        "  \n",
        "# The run() method is called each time a request is made to the scoring API.  \n",
        "def run(data):\n",
        "    json_data = json.loads(data)\n",
        "    input_data = json_data[\"inputs\"][\"article\"]\n",
        "    inputs = TOKENIZER(str(input_data), return_tensors=\"pt\").input_ids\n",
        "\n",
        "    out = MODEL.generate(inputs, max_new_tokens=512, do_sample=False)\n",
        "\n",
        "    summary = TOKENIZER.decode(out[0], skip_special_tokens=True)\n",
        "      \n",
        "    # You can return any JSON-serializable object.  \n",
        "    return {\"summary\": summary}\n",
        "\n",
        "    \n",
        "def test():\n",
        "    # NOTE: You need to comment out model_file/path = os.path.join(...) in init() for local testing\n",
        "    init()\n",
        "    payload = {\n",
        "        \"inputs\": {\n",
        "            \"article\": [\"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"],\n",
        "            \"params\": {\n",
        "                \"max_new_tokens\": 512\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    payload = str.encode(json.dumps(payload))\n",
        "    res = run(payload)\n",
        "    print(res)\n",
        "    \n",
        "    # timed run\n",
        "    start = time.time()\n",
        "    for i in range(10):\n",
        "        _ = run(payload)\n",
        "    diff = time.time() - start\n",
        "    print(f\"time {diff/10} sec\")\n",
        "\n",
        "test()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
