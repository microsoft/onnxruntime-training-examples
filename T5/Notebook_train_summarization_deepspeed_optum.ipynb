{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning T5 model with Azure ML using Azure Container for PyTorch \n",
        "\n",
        "This tutorial shows how to fine tune the T5 model to generate a summary of a news article. We then deploy it to an online endpoint for real time inference. The model is trained on a tiny sample of the dataset with a small number of epochs to illustrate the fine tuning approach.\n",
        "\n",
        "### Learning Objectives\n",
        "- Fine tune the T5 small model for the `Summarization` task with `Azure ML` \n",
        "- Leverage the `ACPT` environment with state of art accelerators\n",
        "- Increase training efficiency using [`DeepSpeed`](https://github.com/microsoft/DeepSpeed) and [`ONNX Runtime`](https://github.com/microsoft/onnxruntime)\n",
        "- Model Evaluation\n",
        "- Register the model with AzureML\n",
        "- Deploy and inference using MIR and ONNX Runtime\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### T5-small is a 60 million parameter model based on text-to-text framework and is used for several NLP tasks, including machine translation, document summarization, question answering pretrained on Colossal Clean Crawled Corpus (C4) dataset.\r\n",
        "\r\n",
        "translation (green), linguistic acceptability (red), sentence similarity (yellow), and document summarization (blue)\r\n",
        "\r\n",
        "##### In this workshop, we will be fine tuning the document summarization task.\r\n",
        "\r\n",
        "\r\n",
        "![Image](assets/t5modelcard.PNG)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Prerequisites to install Azure ML Python SDK Version 2 \r\n",
        "Please restart kernel after pip installs to sync environment with new modules."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install azure-ai-ml azure-identity datasets azure-cli mlflow # fine-tune requirements\r\n",
        "%pip install onnxruntime transformers torch # inference requirements"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Connect to Azure Machine Learning workspace\r\n",
        "\r\n",
        "Before we dive in the code, you'll need to connect to your workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\r\n",
        "\r\n",
        "For this lab, we've already setup an AzureML Workspace for you. If you'd like to learn more about `Workspace`s, please reference [`AzureML's documetation`](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?view=azureml-api-2&tabs=azure-portal).\r\n",
        "\r\n",
        "We are using the `DefaultAzureCredential` to get access to workspace. `DefaultAzureCredential` should be capable of handling most scenarios. If you want to learn more about other available credentials, go to [`Set up authentication`](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk&view=azureml-api-2) for more available credentials."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import (\n",
        "    DefaultAzureCredential,\n",
        "    InteractiveBrowserCredential,\n",
        "    ClientSecretCredential,\n",
        ")\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    credential = InteractiveBrowserCredential()\n",
        "\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential=credential)\n",
        "except:\n",
        "    ml_client = MLClient(\n",
        "        credential,\n",
        "        subscription_id=\"<SubscriptionId>\",\n",
        "        resource_group_name=\"<ResourceGroup>\",\n",
        "        workspace_name=\"<Workspace>\",\n",
        "    )\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684880592658
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Create a compute\r\n",
        "\r\n",
        "Azure Machine Learning needs a compute resource to run a job. This resource can be single or multi-node machines with Linux or Windows OS. In the following example script, we provision a `Standard_ND40rs_v2` SKU which is infiniband enabled to provide higher node communication bandwidth and low latency with mellanox drivers to create an Azure Machine Learning compute. You can get the list and more detail [here](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-hpc#rdma-capable-instances)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\r\n",
        "\r\n",
        "experiment_name = \"T5-Summarization-news-summary\"\r\n",
        "\r\n",
        "# If you already have a gpu cluster, mention it here. Else will create a new one\r\n",
        "compute_cluster = \"Compute-Cluster\"\r\n",
        "try:\r\n",
        "    compute = ml_client.compute.get(compute_cluster)\r\n",
        "    print(\"successfully fetched compute:\", compute.name)\r\n",
        "except Exception as ex:\r\n",
        "    print(\"failed to fetch compute:\", compute_cluster)\r\n",
        "    print(\"creating new Standard_ND40rs_v2 compute\")\r\n",
        "    compute = AmlCompute(\r\n",
        "        name=compute_cluster,\r\n",
        "        size=\"Standard_ND40rs_v2\", # Info on Standard_ND40rs_v2 SKU: https://learn.microsoft.com/en-us/azure/virtual-machines/ndv2-series\r\n",
        "        min_instances=0,\r\n",
        "        max_instances=2,  # For multi node training set this to an integer value more than 1\r\n",
        "    )\r\n",
        "    ml_client.compute.begin_create_or_update(compute).wait()\r\n",
        "    print(\"successfully created compute:\", compute.name)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684880664533
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Create a job environment using Azure Container for Pytorch\r\n",
        "\r\n",
        "We will be creating a custom environment using existing ACPT curated environment consisting of state of art technologies like Deepspeed, OnnxRuntime. You can get more detail from [Custom Environment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-azure-container-for-pytorch-environment?view=azureml-api-2)\r\n",
        "\r\n",
        "\r\n",
        "view the [Environments in Azure Machine Learning studio](https://ml.azure.com/environments)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment, BuildContext\r\n",
        "\r\n",
        "Env_Name = \"MSBuildLab110_env\"\r\n",
        "env_docker_context = Environment(\r\n",
        "    build=BuildContext(path=\"src/Environment/context\"),\r\n",
        "    name=Env_Name,\r\n",
        "    description=\"Environment created from a Docker context.\",\r\n",
        ")\r\n",
        "ml_client.environments.create_or_update(env_docker_context)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684880696598
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### 5. Pick the dataset for fine-tuning the model\n",
        "\n",
        "The [CNN DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. It is larger than 1GB when uncompressed. \n",
        "\n",
        "We want this sample to run quickly, so a copy of the fraction of dataset is used for fine tuning job.This means the fine tuned model will have lower accuracy, hence it should not be put to real-world use. \n",
        "* Visualize some data rows. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"cnn_dailymail\"\n",
        "import pandas as pd\n",
        "pd.set_option(\n",
        "    \"display.max_colwidth\", 1000\n",
        ")\n",
        "train_df = pd.read_json(\"./src/Finetune/cnn_daily.jsonl\", lines=True)\n",
        "train_df.head(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684880792669
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Finetune the T5 small model for Summarization task\r\n",
        "\r\n",
        "Leveraging Deepspeed and Onnxruntime accelarators for improving the efficiency for memory and compute and in turn reduce the training cost. \r\n",
        "\r\n",
        "The table below details some of the parameters passed to the training job.\r\n",
        "\r\n",
        "| Parameters/accelarators | Description |\r\n",
        "| ----------------- | --- |\r\n",
        "| model_name | The name of the model getting finetuned. Here we specify T5-small. |\r\n",
        "| ort | [Onnx runtime](https://github.com/microsoft/onnxruntime) accelarates 2x speed up in training time for SOTA models and optimizes memory to fit larger model such as GPT3 on 16GB GPU which would otherwise run out of mmemory. |\r\n",
        "| deepspeed | [Deepspeed](https://github.com/microsoft/deepspeed) enables running billions of parameter models distributed across GPUs and provide different stages for memory and compute efficiency. |\r\n",
        "| number of epochs | 1 |\r\n",
        "| max train samples | 10 |\r\n",
        "| Nebula | checkpointing |\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command, Input, Output\r\n",
        "from azure.ai.ml.entities import Data\r\n",
        "from azure.ai.ml.constants import AssetTypes\r\n",
        "\r\n",
        "from azure.ai.ml.entities import (\r\n",
        "    VsCodeJobService,\r\n",
        "    TensorBoardJobService,\r\n",
        "    JupyterLabJobService,\r\n",
        ")\r\n",
        "\r\n",
        "job = command(\r\n",
        "    code=\".\",\r\n",
        "    command=\"python src/Finetune/train_summarization_deepspeed_optum.py \\\r\n",
        "        --deepspeed src/Finetune/ds_config.json \\\r\n",
        "        --model_name_or_path t5-small \\\r\n",
        "        --dataset_name cnn_dailymail \\\r\n",
        "        --max_train_samples=10 \\\r\n",
        "        --max_eval_samples=10 \\\r\n",
        "        --dataset_config '3.0.0' \\\r\n",
        "        --do_train \\\r\n",
        "        --num_train_epochs=1 \\\r\n",
        "        --per_device_train_batch_size=16 \\\r\n",
        "        --per_device_eval_batch_size=16  \\\r\n",
        "        --output_dir outputs \\\r\n",
        "        --overwrite_output_dir \\\r\n",
        "        --fp16 \\\r\n",
        "        --optim adamw_ort_fused\",\r\n",
        "    compute=compute_cluster,\r\n",
        "    services={\r\n",
        "      \"My_jupyterlab\": JupyterLabJobService(\r\n",
        "        nodes=\"all\" # For distributed jobs, use the `nodes` property to pick which node you want to enable interactive services on. If `nodes` are not selected, by default, interactive applications are only enabled on the head node. Values are \"all\", or compute node index (for ex. \"0\", \"1\" etc.)\r\n",
        "      ),\r\n",
        "      \"My_vscode\": VsCodeJobService(\r\n",
        "        nodes=\"all\"\r\n",
        "      ),\r\n",
        "      \"My_tensorboard\": TensorBoardJobService(\r\n",
        "        nodes=\"all\",\r\n",
        "        log_dir=\"outputs/runs\"  # relative path of Tensorboard logs (same as in your training script)         \r\n",
        "      ),\r\n",
        "    },\r\n",
        "    environment=Environment(name=\"acpt-env-msbuild\", image=\"azuremlsamples.azurecr.io/msbuild110/acpt-t5:latest\"),\r\n",
        "    instance_count=1,  \r\n",
        "    distribution={\r\n",
        "        \"type\": \"PyTorch\",\r\n",
        "        \"process_count_per_instance\": 8,\r\n",
        "    },\r\n",
        ") # basic environment comes with my workspace\r\n",
        "\r\n",
        "job = ml_client.jobs.create_or_update(job)\r\n",
        "job.studio_url"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684880890604
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results show **300%** improvement of Fine-tune job with 100 epoch and CNN_Daily dataset with ORT, Deepspeed and Nebula checkpointing"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Image](assets/performance_100epoch.PNG)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "![Image](assets/Noaccelarator.PNG)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "![Image](assets/dsandort.PNG)\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Note**: Wait until the training run from previous cell completes\r\n",
        "#### 7. Register the fine tuned model with the workspace\r\n",
        "\r\n",
        "We will register the model from the output of the fine tuning job. This will track lineage between the fine tuned model and the fine tuning job. The fine tuning job, further, tracks lineage to the foundation model, data and training code."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To register the example model, follow these steps:\r\n",
        "\r\n",
        "1. Go to the [Azure Machine Learning studio](https://ml.azure.com).\r\n",
        "1. In the left navigation bar, select the **Models** page.\r\n",
        "1. Select **Register**, and then choose **From job output** and choose your model.\r\n",
        "1. Select __Unspecified type__ for the __Model type__.\r\n",
        "1. Select the output path where model is located in your job\r\n",
        "1. Enter a friendly __Name__ for the model. The steps in this article assume the model is named `model-1`.\r\n",
        "1. Select __Next__, and then __Register__ to complete registration."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Model\r\n",
        "from azure.ai.ml.constants import AssetTypes\r\n",
        "import time\r\n",
        "\r\n",
        "timestamp = str(int(time.time()))\r\n",
        "model_name = \"T5Model\"\r\n",
        "\r\n",
        "#MLFlow model registration\r\n",
        "mlflow_modelpath = \"azureml://jobs/{jobname}/outputs/artifacts/outputs/mlflow\".format(jobname = job.name)\r\n",
        "cloud_model = Model(\r\n",
        "    path=mlflow_modelpath,\r\n",
        "    name=model_name+\"_mlflow\",\r\n",
        "    type=AssetTypes.MLFLOW_MODEL,\r\n",
        "    description=\"Model created from cloud path.\",\r\n",
        "    version=timestamp,\r\n",
        ")\r\n",
        "ml_client.models.create_or_update(cloud_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684881570590
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Model Evaluation\r\n",
        "The goal of evaluating model is to compare their performance on a variety of metrics. text-summarization is generic task type that can be used for scenarios such as abstractive and extractive summarization. \r\n",
        "\r\n",
        "We will create the job that uses the model_evaluation_pipeline component and submit for the registered model.\r\n",
        "\r\n",
        "Note that the metrics that the evaluation job calculate are **rouge1, rouge2, rougeL and rougeLsum** in this sample.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 8.1 Fetch the prebuilt fine tuning model evaluation component"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.dsl import pipeline\r\n",
        "from azure.ai.ml import Input\r\n",
        "from azure.ai.ml.constants import AssetTypes\r\n",
        "from azure.ai.ml.dsl import pipeline\r\n",
        "from azure.ai.ml import load_component\r\n",
        "import time\r\n",
        "\r\n",
        "\r\n",
        "test_data = \"src/Finetune/small_test-inference.jsonl\"\r\n",
        "\r\n",
        "# fetch the pipeline component\r\n",
        "registry = \"azureml\"\r\n",
        "subscription_id = ml_client.subscription_id\r\n",
        "resource_group = ml_client.resource_group_name\r\n",
        "\r\n",
        "registry = \"azureml\"\r\n",
        "\r\n",
        "registry_ml_client = MLClient(\r\n",
        "    credential, subscription_id, resource_group, registry_name=registry\r\n",
        ")\r\n",
        "\r\n",
        "pipeline_component_func = registry_ml_client.components.get(\r\n",
        "    name=\"model_evaluation_pipeline\", label=\"latest\"\r\n",
        ")\r\n",
        "model = ml_client.models.get(name=model_name+\"_mlflow\", version = timestamp)\r\n",
        "\r\n",
        "# define the pipeline job\r\n",
        "@pipeline()\r\n",
        "def evaluation_pipeline(mlflow_model):\r\n",
        "    evaluation_job = pipeline_component_func(\r\n",
        "        # specify the foundation model available in the azureml system registry or a model from the workspace\r\n",
        "        # mlflow_model = Input(type=AssetTypes.MLFLOW_MODEL, path=f\"{mlflow_model_path}\"),\r\n",
        "        mlflow_model=mlflow_model,\r\n",
        "        # test data\r\n",
        "        test_data=Input(type=AssetTypes.URI_FILE, path=test_data),\r\n",
        "        # The following parameters map to the dataset fields\r\n",
        "        input_column_names=\"article\",\r\n",
        "        label_column_name=\"highlights\",\r\n",
        "        # Evaluation settings\r\n",
        "        task=\"text-summarization\",\r\n",
        "        device=\"gpu\",\r\n",
        "    )\r\n",
        "    return {\"evaluation_result\": evaluation_job.outputs.evaluation_result}\r\n",
        "\r\n",
        "# submit the pipeline job for each model that we want to evaluate\r\n",
        "# you could consider submitting the pipeline jobs in parallel, provided your cluster has multiple nodes\r\n",
        "\r\n",
        "pipeline_jobs = []\r\n",
        "\r\n",
        "\r\n",
        "pipeline_object = evaluation_pipeline(\r\n",
        "    mlflow_model=Input(type=AssetTypes.MLFLOW_MODEL, path=f\"{model.id}\"),\r\n",
        ")\r\n",
        "# don't reuse cached results from previous jobs\r\n",
        "pipeline_object.settings.force_rerun = True\r\n",
        "pipeline_object.settings.default_compute = compute_cluster\r\n",
        "pipeline_object.display_name = f\"eval-{model.name}-{timestamp}\"\r\n",
        "pipeline_job = ml_client.jobs.create_or_update(\r\n",
        "    pipeline_object, experiment_name=experiment_name\r\n",
        ")\r\n",
        "# add model['name'] and pipeline_job.name as key value pairs to a dictionary\r\n",
        "pipeline_jobs.append({\"model_name\": model.name, \"job_name\": pipeline_job.name})\r\n",
        "\r\n",
        "evaljob = ml_client.jobs.create_or_update(pipeline_job)\r\n",
        "evaljob.studio_url\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684881646594
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Image](assets/modelevaluation.png)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Operationalizing the model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 9.1 Register Onnx model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp = str(int(time.time()))\r\n",
        "model_name = \"T5Model\"\r\n",
        "\r\n",
        "#Onnx model registration\r\n",
        "modelpath = \"azureml://jobs/{jobname}/outputs/artifacts/outputs/onnx\".format(jobname = job.name)\r\n",
        "cloud_model = Model(\r\n",
        "    path=modelpath,\r\n",
        "    name=model_name+\"_onnx\",\r\n",
        "    type=AssetTypes.CUSTOM_MODEL,\r\n",
        "    description=\"Model created from cloud path.\",\r\n",
        "    version=timestamp,\r\n",
        ")\r\n",
        "ml_client.models.create_or_update(cloud_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684449853942
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 9.2 Create online endpoint\r\n",
        "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import (\r\n",
        "    ManagedOnlineEndpoint,\r\n",
        "    ManagedOnlineDeployment,\r\n",
        "    Model,\r\n",
        "    Environment,\r\n",
        "    CodeConfiguration,\r\n",
        ")\r\n",
        "# Define an endpoint name\r\n",
        "endpoint_name = \"MSBuildLab110_endpoint\"\r\n",
        "\r\n",
        "# Example way to define a random name\r\n",
        "import datetime\r\n",
        "\r\n",
        "endpoint_name = \"endpt-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\r\n",
        "\r\n",
        "# create an online endpoint\r\n",
        "endpoint = ManagedOnlineEndpoint(\r\n",
        "    name = endpoint_name, \r\n",
        "    description=\"this is a endpoint for T5 summarization model\",\r\n",
        "    auth_mode=\"key\"\r\n",
        ")\r\n",
        "\r\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).wait()\r\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684450035960
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 9.3 Deploy scoring file to the endpoint"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment(\r\n",
        "    image=\"mcr.microsoft.com/azureml/curated/acpt-t5:latest\",\r\n",
        ")\r\n",
        "\r\n",
        "model = ml_client.models.get(name=model_name+\"_onnx\", version = timestamp)\r\n",
        "\r\n",
        "blue_deployment = ManagedOnlineDeployment(\r\n",
        "    name=\"blue\",\r\n",
        "    endpoint_name=endpoint_name,\r\n",
        "    model=model,\r\n",
        "    environment=env,\r\n",
        "    code_configuration=CodeConfiguration(\r\n",
        "        code=\".\", scoring_script=\"src/Operationalize/score_onnx.py\"\r\n",
        "    ),\r\n",
        "    instance_type=\"Standard_F8s_v2\",\r\n",
        "    instance_count=1,\r\n",
        ")\r\n",
        "\r\n",
        "ml_client.online_deployments.begin_create_or_update(blue_deployment)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684450079954
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aside: Scoring files for ONNX Runtime Inference vs. Hugging Face Inference\r\n",
        "\r\n",
        "![Image](assets/T5_beamsearch.png)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json \r\n",
        "import numpy as np\r\n",
        "from onnxruntime import InferenceSession\r\n",
        "import os\r\n",
        "import time\r\n",
        "from transformers import AutoTokenizer\r\n",
        "\r\n",
        "# Documentation: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints\r\n",
        "# Troubleshooting: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints\r\n",
        "  \r\n",
        "# The init() method is called once, when the web service starts up.\r\n",
        "def init():  \r\n",
        "    global SESS\r\n",
        "    global TOKENIZER\r\n",
        "    # The AZUREML_MODEL_DIR environment variable indicates  \r\n",
        "    # a directory containing the model file you registered.  \r\n",
        "    # model_filename = os.path.join(os.environ['AZUREML_MODEL_DIR'], \"onnx/outputs_beam_search.onnx\")  \r\n",
        "\r\n",
        "    model_filename = \"src/Model/onnx/outputs_beam_search.onnx\" \r\n",
        "    SESS = InferenceSession(model_filename, providers=[\"CPUExecutionProvider\"])\r\n",
        "\r\n",
        "    TOKENIZER = AutoTokenizer.from_pretrained(\"t5-small\")\r\n",
        "  \r\n",
        "# The run() method is called each time a request is made to the scoring API.  \r\n",
        "def run(data):\r\n",
        "    json_data = json.loads(data)\r\n",
        "    input_data = json_data[\"inputs\"][\"article\"]\r\n",
        "    \r\n",
        "    input_ids = TOKENIZER(str(input_data), return_tensors=\"pt\").input_ids\r\n",
        "\r\n",
        "    ort_inputs = {\r\n",
        "        \"input_ids\": np.array(input_ids, dtype=np.int32),\r\n",
        "        \"max_length\": np.array([512], dtype=np.int32),\r\n",
        "        \"min_length\": np.array([0], dtype=np.int32),\r\n",
        "        \"num_beams\": np.array([1], dtype=np.int32),\r\n",
        "        \"num_return_sequences\": np.array([1], dtype=np.int32),\r\n",
        "        \"length_penalty\": np.array([1.0], dtype=np.float32),\r\n",
        "        \"repetition_penalty\": np.array([1.0], dtype=np.float32)\r\n",
        "    }\r\n",
        "    \r\n",
        "    out = SESS.run(None, ort_inputs)[0][0] # 0th batch, 0th sample\r\n",
        "\r\n",
        "    summary = TOKENIZER.decode(out[0], skip_special_tokens=True)\r\n",
        "\r\n",
        "    # You can return any JSON-serializable object.\r\n",
        "    return {\"summary\": summary}\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    # NOTE: You need to comment out model_filename = os.path.join(...) in init() for local testing\r\n",
        "    init()\r\n",
        "    payload = {\r\n",
        "        \"inputs\": {\r\n",
        "            \"article\": [\"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"],\r\n",
        "            \"params\": {\r\n",
        "                \"max_new_tokens\": 512\r\n",
        "            }\r\n",
        "        }\r\n",
        "    }\r\n",
        "    payload = str.encode(json.dumps(payload))\r\n",
        "    res = run(payload)\r\n",
        "    print(res)\r\n",
        "\r\n",
        "    # timed run\r\n",
        "    start = time.time()\r\n",
        "    for i in range(10):\r\n",
        "        _ = run(payload)\r\n",
        "    diff = time.time() - start\r\n",
        "    print(f\"time {diff/10} sec\")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684881852579
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\r\n",
        "import os\r\n",
        "from transformers import pipeline\r\n",
        "import json \r\n",
        "import time\r\n",
        "import joblib\r\n",
        "from transformers import AutoTokenizer, AutoConfig\r\n",
        "from transformers import AutoModelForSeq2SeqLM\r\n",
        "import torch\r\n",
        "\r\n",
        "# Documentation: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints\r\n",
        "# Troubleshooting: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints\r\n",
        "  \r\n",
        "# The init() method is called once, when the web service starts up.\r\n",
        "def init():  \r\n",
        "    global MODEL\r\n",
        "    global TOKENIZER\r\n",
        "    # The AZUREML_MODEL_DIR environment variable indicates  \r\n",
        "    # a directory containing the model file you registered.  \r\n",
        "    # model_path = os.path.join(os.environ['AZUREML_MODEL_DIR'])\r\n",
        "    # model_file = os.path.join(os.environ['AZUREML_MODEL_DIR'], \"pytorch_model.bin\")\r\n",
        "\r\n",
        "    model_path = \"src/Model\"\r\n",
        "    model_file = \"src/Model/pytorch_model.bin\"\r\n",
        "    TOKENIZER = AutoTokenizer.from_pretrained(model_path)\r\n",
        "    config = AutoConfig.from_pretrained(model_path)\r\n",
        "    MODEL = AutoModelForSeq2SeqLM.from_pretrained(model_file, config=config) \r\n",
        "    \r\n",
        "  \r\n",
        "# The run() method is called each time a request is made to the scoring API.  \r\n",
        "def run(data):\r\n",
        "    json_data = json.loads(data)\r\n",
        "    input_data = json_data[\"inputs\"][\"article\"]\r\n",
        "    inputs = TOKENIZER(str(input_data), return_tensors=\"pt\").input_ids\r\n",
        "\r\n",
        "    out = MODEL.generate(inputs, max_new_tokens=512, do_sample=False)\r\n",
        "\r\n",
        "    summary = TOKENIZER.decode(out[0], skip_special_tokens=True)\r\n",
        "      \r\n",
        "    # You can return any JSON-serializable object.  \r\n",
        "    return {\"summary\": summary}\r\n",
        "\r\n",
        "    \r\n",
        "if __name__ == '__main__':\r\n",
        "    # NOTE: You need to comment out model_file/path = os.path.join(...) in init() for local testing\r\n",
        "    init()\r\n",
        "    payload = {\r\n",
        "        \"inputs\": {\r\n",
        "            \"article\": [\"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"],\r\n",
        "            \"params\": {\r\n",
        "                \"max_new_tokens\": 512\r\n",
        "            }\r\n",
        "        }\r\n",
        "    }\r\n",
        "    payload = str.encode(json.dumps(payload))\r\n",
        "    res = run(payload)\r\n",
        "    print(res)\r\n",
        "    \r\n",
        "    # timed run\r\n",
        "    start = time.time()\r\n",
        "    for i in range(10):\r\n",
        "        _ = run(payload)\r\n",
        "    diff = time.time() - start\r\n",
        "    print(f\"time {diff/10} sec\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684881900556
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 9.4: Invoke the endpoint to score data by using your model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the blue deployment with some sample data\r\n",
        "ml_client.online_endpoints.invoke(\r\n",
        "    endpoint_name=endpoint_name,\r\n",
        "    deployment_name=\"blue\",\r\n",
        "    request_file=\"src/Operationalize/payload.json\",\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 9.5: Delete the online endpoint\r\n",
        "Don't forget to delete the online endpoint, else you will leave the billing meter running for the compute used by the endpoint"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.begin_delete(name=endpoint_name).wait()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684306030654
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}