{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate pretraining of BERT model using ONNX Runtime\n",
    "This notebook contains a walkthrough of using ONNX Runtime in Azure Machine Learning service to pretrain [BERT: Bidirectional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805) models. This example shows how ONNX Runtime training can accelerate BERT pretraining implementation in PyTorch maintained at https://github.com/NVIDIA/DeepLearningExamples.\n",
    "\n",
    "Steps:\n",
    "- Intialize an AzureML workspace\n",
    "- Register a datastore to use preprocessed data for training\n",
    "- Create an AzureML experiment\n",
    "- Provision a compute target\n",
    "- Create an Estimator\n",
    "- Configure and Run\n",
    "\n",
    "Prerequisites\n",
    "If you are using an Azure Machine Learning [Compute Instance](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-instance) you are all set. Otherwise, you need to setup your environment by installing AzureML Python SDK to run this notebook. Refer to [How to use Estimator in Azure ML](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/how-to-use-estimator/how-to-use-estimator.ipynb) notebook first if you haven't already to establish your connection to the AzureML Workspace. \n",
    "\n",
    "Refer to instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/nvidia-bert/README.md before running the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check SDK installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# AzureML libraries\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Datastore, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "from azureml.core.runconfig import MpiConfiguration, RunConfiguration, DEFAULT_GPU_IMAGE\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AzureML Workspace setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or retrieve Azure machine learning workspace\n",
    "# see https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py\n",
    "ws = Workspace.get(name=\"myworkspace\", subscription_id='<azure-subscription-id>', resource_group='myresourcegroup')\n",
    "\n",
    "# Print workspace attributes\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Workspace region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Datastore\n",
    "Before running the step below, data prepared using the instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/nvidia-bert/README.md should be transferred to an Azure Blob container referenced in the `Datastore` registration step. Refer to the documentation at https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data for details on using data in Azure ML experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datastore from blob storage containing training data.\n",
    "# Consult README.md for instructions downloading and uploading training data.\n",
    "ds = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                             datastore_name='<datastore-name>',\n",
    "                                             account_name='<storage-account-name>', \n",
    "                                             account_key='<storage-account-key>',\n",
    "                                             container_name='<storage-container-name>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print datastore attributes\n",
    "print('Datastore name: ' + ds.name, \n",
    "      'Container name: ' + ds.container_name, \n",
    "      'Datastore type: ' + ds.datastore_type, \n",
    "      'Workspace name: ' + ds.workspace.name, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AzureML Compute Cluster\n",
    "This recipe is supported on Azure Machine Learning Service using 16 x Standard_NC24rs_v3 or 8 x Standard_ND40rs_v2 VMs. In the next step, you will create an AzureML Compute cluster of Standard_NC40s_v2 GPU VMs with the specified name, if it doesn't already exist in your workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GPU cluster\n",
    "gpu_cluster_name = \"ortbertpretrain\" \n",
    "try:\n",
    "    gpu_compute_target = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_ND40rs_v2', min_nodes=0, max_nodes=8)\n",
    "    gpu_compute_target = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "    gpu_compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment for phase 1\n",
    "experiment_name = 'nvbert-ort-pretraining-phase1'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator\n",
    "Notes before running the following step:\n",
    "* Update the following step to replace two occurences of `<blob-path-to-phase1-training-data>` with the actual path in the datastore that contains the training files.\n",
    "* If you followed instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/nvidia-bert/README.md to prepare data, make sure that the data and others files that are not code or config are moved out `workspace` directory. Data files should have been moved to a `Datastore` to use in training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this directory should contain run_pretraining_ort.py, ort_supplement directory and other files copied over\n",
    "# based on the instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/nvidia-bert/README.md \n",
    "project_folder = '../../workspace'\n",
    "\n",
    "# see README.md for instructions pushing Docker image with onnxruntime build\n",
    "image_name = 'onnxruntime-training:0.1-rc1-openmpi4.0-cuda10.1-cudnn7.6-nccl2.4.8'\n",
    "\n",
    "# credentials to registry containing above Docker image\n",
    "cr = ContainerRegistry()\n",
    "cr.address = '<registry-name>.azurecr.io'\n",
    "\n",
    "# set MPI configuration\n",
    "# set processes per node to be equal to GPU count on SKU.\n",
    "mpi = MpiConfiguration()\n",
    "mpi.process_count_per_node = 8\n",
    "\n",
    "import uuid\n",
    "output_id = uuid.uuid1().hex\n",
    "\n",
    "# Define training estimator for phase 1\n",
    "# Consult https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-ml-models\n",
    "# Fill in blob path to phase 1 training data in argument below\n",
    "estimator_ph1 = Estimator(source_directory=project_folder,\n",
    "\n",
    "                    # Compute configuration\n",
    "                    compute_target = gpu_compute_target,\n",
    "                    node_count=4,\n",
    "                    process_count_per_node=1,  # separate MPI jobs\n",
    "                    distributed_training = mpi,\n",
    "                    use_gpu = True,\n",
    "                    \n",
    "                    # supply Docker image\n",
    "                    use_docker = True,\n",
    "                    custom_docker_image = image_name,\n",
    "                    image_registry_details = cr,\n",
    "                    user_managed = True,\n",
    "                    \n",
    "                    # Training script parameters\n",
    "                    script_params = {\n",
    "                        \"--config_file\": \"bert_config.json\",\n",
    "                        '--input_dir' : ds.path('<blob-path-to-phase1-training-data>').as_mount(), \n",
    "                        '--output_dir': ds.path(f'output/{experiment_name}/{output_id}/').as_mount(),\n",
    "                        '--bert_model' : 'bert-large-uncased',\n",
    "                        '--train_batch_size' : 4096,\n",
    "                        '--max_seq_length': 128,\n",
    "                        '--max_predictions_per_seq': 20,\n",
    "                        '--max_steps' : 7038,\n",
    "                        '--warmup_proportion' : '0.2843',\n",
    "                        '--num_steps_per_checkpoint' : 200,\n",
    "                        '--learning_rate' : '6e-3',\n",
    "                        '--seed': 42,\n",
    "                        '--fp16' : '',\n",
    "                        '--gradient_accumulation_steps' : 32,\n",
    "                        '--allreduce_post_accumulation' : '',\n",
    "                        '--allreduce_post_accumulation_fp16' : '',\n",
    "                        '--do_train' : '',\n",
    "                        '--use_ib' : '', # pass if infiniband available on SKU\n",
    "                        '--gpu_memory_limit_gb' : 32 # set to per GPU memory in GB (check SKU)\n",
    "                    },\n",
    "                    \n",
    "                    entry_script = 'run_pretraining_ort.py',\n",
    "                    inputs = [ds.path('<blob-path-to-phase1-training-data>').as_mount()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run AzureML experiment - Phase 1 of pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit phase 1 (check logs from Outputs + logs tab of corresponding link)\n",
    "run = experiment.submit(estimator_ph1)\n",
    "RunDetails(run).show()\n",
    "print(run.get_portal_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment for phase 2\n",
    "experiment_name = 'nvbert-ort-pretraining-phase2'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator - Phase 2\n",
    "Notes before running the following step:\n",
    "* Update the following step to replace two occurences of `<blob-path-to-phase1-training-data>` with the actual path in the datastore that contains the training files.\n",
    "* If you followed instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/nvidia-bert/README.md to prepare data, make sure that the data and others files that are not code or config are moved out `workspace` directory. Data files should have been moved to a `Datastore` to use in training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training estimator for phase 2\n",
    "# Fill in blob path to phase 1 training data as well as phase 1 checkpoint in arguments below\n",
    "estimator_ph2 = Estimator(source_directory=project_folder,\n",
    "\n",
    "                    # Compute configuration\n",
    "                    compute_target = gpu_compute_target,\n",
    "                    node_count=4, \n",
    "                    process_count_per_node=1, # separate MPI jobs\n",
    "                    distributed_training = mpi,\n",
    "                    use_gpu = True,\n",
    "                    \n",
    "                    #Docker image\n",
    "                    use_docker = True,\n",
    "                    custom_docker_image = image_name,\n",
    "                    image_registry_details = cr,\n",
    "                    user_managed = True,\n",
    "                    \n",
    "                    # Training script parameters\n",
    "                    script_params = {\n",
    "                        # Required Params\n",
    "                        \"--config_file\": \"bert_config.json\",\n",
    "                        '--input_dir' : ds.path('<blob-path-to-phase2-training-data>').as_mount(), \n",
    "                        '--output_dir': ds.path(f'output/{experiment_name}/{output_id}/').as_mount(),\n",
    "                        '--bert_model' : 'bert-large-uncased',\n",
    "                        '--train_batch_size' : 4096,\n",
    "                        '--max_seq_length': 512,\n",
    "                        '--max_predictions_per_seq': 80,\n",
    "                        '--max_steps' : 1563,\n",
    "                        '--warmup_proportion' : '0.128',\n",
    "                        '--num_steps_per_checkpoint' : 200,\n",
    "                        '--learning_rate' : '4e-3',\n",
    "                        '--seed': 42,\n",
    "                        '--fp16' : '',\n",
    "                        '--gradient_accumulation_steps' : 256,\n",
    "                        '--allreduce_post_accumulation' : '',\n",
    "                        '--allreduce_post_accumulation_fp16' : '',\n",
    "                        '--do_train' : '',\n",
    "                        '--phase2' : '',\n",
    "                        '--resume_from_checkpoint' : '',\n",
    "                        '--phase1_end_step' : '7038',\n",
    "                        '--init_checkpoint' : ds.path('<path-to-checkpoint-from-phase-1>'),\n",
    "                        '--use_ib' : '', # pass if infiniband available on SKU\n",
    "                        '--gpu_memory_limit_gb' : 32 # set to per GPU memory in GB (check SKU)\n",
    "                    },\n",
    "                    \n",
    "                    entry_script='run_pretraining_ort.py',\n",
    "                    inputs=[ds.path('<blob-path-to-phase2-training-data>').as_mount()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run AzureML experiment - Phase 2 of pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit phase 2 run (check logs from Outputs + logs tab of corresponding link)\n",
    "run = experiment.submit(estimator_ph2)\n",
    "RunDetails(run).show()\n",
    "print(run.get_portal_url())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
