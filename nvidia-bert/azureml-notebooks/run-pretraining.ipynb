{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate pretraining of BERT model using ONNX Runtime\n",
    "This notebook contains a walkthrough of using ONNX Runtime in Azure Machine Learning service to pretrain [BERT: Bidirectional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805) models. This example shows how the ONNX Runtime training can accelerate the BERT pretraining implementation in PyTorch maintained at https://github.com/NVIDIA/DeepLearningExamples.\n",
    "\n",
    "Steps:\n",
    "- Intialize an AzureML workspace\n",
    "- Register a datastore to use preprocessed data for training\n",
    "- Create an AzureML experiment\n",
    "- Provision a compute target\n",
    "- Create an Estimator\n",
    "- Configure and Run\n",
    "\n",
    "Prerequisites:\n",
    "If you are using an Azure Machine Learning [Compute Instance](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-instance) you are all set. Otherwise, you need to setup your environment by installing the AzureML Python SDK to run this notebook. Refer to [How to use Estimator in Azure ML](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/how-to-use-estimator/how-to-use-estimator.ipynb) first if you haven't already to established an AzureML Workspace. \n",
    "\n",
    "Refer to instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/nvidia-bert/README.md before running the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check SDK installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# AzureML libraries\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Datastore, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "from azureml.core.runconfig import MpiConfiguration, RunConfiguration, DEFAULT_GPU_IMAGE\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.train.estimator import Estimator\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AzureML Workspace setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or retrieve Azure machine learning workspace\n",
    "# see https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py\n",
    "ws = Workspace.get(name=\"myworkspace\", subscription_id='<azure-subscription-id>', resource_group='myresourcegroup')\n",
    "\n",
    "# Print workspace attributes\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Workspace region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Datastore\n",
    "Before running the step below, the training data needs to be made available in an Azure Blob container. Refer to  https://github.com/microsoft/onnxruntime-training-examples/blob/master/nvidia-bert/README.md for detailed instructions on preparing the training data. Be advised the data preparation is a time-consuming process. \n",
    "\n",
    "For additional documentation on accessing data using Azure machine learning, see https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datastore from blob storage containing training data.\n",
    "# Consult README.md for instructions downloading and uploading training data.\n",
    "ds = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                             datastore_name='<datastore-name>',\n",
    "                                             account_name='<storage-account-name>', \n",
    "                                             account_key='<storage-account-key>',\n",
    "                                             container_name='<storage-container-name>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print datastore attributes\n",
    "print('Datastore name: ' + ds.name, \n",
    "      'Container name: ' + ds.container_name, \n",
    "      'Datastore type: ' + ds.datastore_type, \n",
    "      'Workspace name: ' + ds.workspace.name, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AzureML Compute Cluster\n",
    "This recipe is supported on Azure Machine Learning Service using 16 x Standard_NC24rs_v3 or 8 x Standard_ND40rs_v2 VMs. In the next step, you will create an AzureML Compute cluster of Standard_NC40s_v2 GPU VMs with the specified name, if it doesn't already exist in your workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GPU cluster\n",
    "gpu_cluster_name = \"ortbertpretrain\" \n",
    "try:\n",
    "    gpu_compute_target = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_ND40rs_v2', min_nodes=0, max_nodes=8)\n",
    "    gpu_compute_target = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "    gpu_compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment for phase 1\n",
    "experiment_name = 'nvbert-ort-pretraining-phase1'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator\n",
    "Notes before running the following step:\n",
    "* Update the following step to replace two occurences of `<blob-path-to-phase1-training-data>` with the actual path in the datastore that contains the training files.\n",
    "\n",
    "To fully utilize capacity, we suggest parameters from below table for phase 1. \n",
    "\n",
    "| VM SKU             | node_count         | global_batch_size         | gpu_feed_batch_size | gradient_accumulation_passes |\n",
    "| ------------------ |:------------------:|-----------------:|-----------------:| ---------------------------:|\n",
    "| Standard_ND40rs_v2 | 1 (8 GPUs total)   | 65,536  | 128  | 64  |\n",
    "| Standard_ND40rs_v2 | 2 (16 GPUs total)  | 65,536  | 128  | 32  |\n",
    "| Standard_ND40rs_v2 | 4 (32 GPUs total)  | 65,536  | 128  | 16  |\n",
    "| Standard_ND40rs_v2 | 8 (64 GPUs total)  | 65,536  | 128  | 8   |\n",
    "| Standard_NC24rs_v3 | 1 (4 GPUs total)   | 65,280  | 48 | 340 |\n",
    "| Standard_NC24rs_v3 | 2 (8 GPUs total)   | 65,280  | 48  | 170 |\n",
    "| Standard_NC24rs_v3 | 4 (16 GPUs total)  | 65,280  | 48  | 85  |\n",
    "| Standard_NC24rs_v3 | 8 (32 GPUs total)  | 64,512  | 48  | 42  |\n",
    "| Standard_NC24rs_v3 | 16 (64 GPUs total) | 64,512  | 48  | 21  |\n",
    "\n",
    "Refer to [README.md](../README.md) for further explanation of batch sizes and gradient accumulation passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this directory should contain run_pretraining.py, bert_config.json, and the bert_runner directory\n",
    "project_folder = '..'\n",
    "\n",
    "image_name = 'mcr.microsoft.com/azureml/onnxruntime-training:0.1-rc3-openmpi4.0-cuda10.2-cudnn8.0-nccl2.7-for-bert'\n",
    "\n",
    "# set MPI configuration\n",
    "# set processes per node to be equal to GPU count on SKU.\n",
    "mpi = MpiConfiguration()\n",
    "mpi.process_count_per_node = 8\n",
    "\n",
    "import uuid\n",
    "output_id = uuid.uuid1().hex\n",
    "\n",
    "# Define training estimator for phase 1\n",
    "# Consult https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-ml-models\n",
    "# Fill in blob path to phase 1 training data in argument below\n",
    "estimator_ph1 = Estimator(source_directory=project_folder,\n",
    "\n",
    "                    # Compute configuration\n",
    "                    compute_target = gpu_compute_target,\n",
    "                    node_count=4,\n",
    "                    process_count_per_node=1, # each counts as a separate MPI job\n",
    "                    distributed_training = mpi,\n",
    "                    use_gpu = True,\n",
    "                    \n",
    "                    # Supply Docker image\n",
    "                    use_docker = True,\n",
    "                    custom_docker_image = image_name,\n",
    "                    user_managed = True,\n",
    "\n",
    "                    # Training script parameters\n",
    "                    script_params = {\n",
    "                        '--config_file': 'bert_config.json',\n",
    "                        '--data_dir' :  ds.path('<blob-path-to-phase1-training-data>').as_mount(), \n",
    "                        '--output_dir': ds.path(f'output/{experiment_name}/{output_id}/').as_mount(),\n",
    "                        '--max_seq_length': 128,\n",
    "                        '--max_predictions_per_seq': 20,                        \n",
    "                        '--gpu_feed_batch_size' : 128,\n",
    "                        '--gradient_accumulation_passes' : 16,\n",
    "                        '--allreduce_post_accumulation' : '',                             \n",
    "                        '--fp16' : '',                 \n",
    "                        '--max_steps' : 7038,\n",
    "                        '--num_steps_per_checkpoint' : 200,\n",
    "                        '--num_passes_to_smooth_output' : 32,\n",
    "                        '--learning_rate' : '6e-3',\n",
    "                        '--warmup_proportion' : '0.2843',\n",
    "                        '--seed': 42,\n",
    "                    },\n",
    "                    \n",
    "                    entry_script = 'run_pretraining.py',\n",
    "                    inputs = [ds.path('<blob-path-to-phase1-training-data>').as_mount()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run AzureML experiment - Phase 1 of pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit phase 1 (check logs from Outputs + logs tab of corresponding link)\n",
    "run = experiment.submit(estimator_ph1)\n",
    "print(run.get_portal_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment for phase 2\n",
    "experiment_name = 'nvbert-ort-pretraining-phase2'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator - Phase 2\n",
    "Notes before running the following step:\n",
    "* Update the following step to replace two occurences of `<blob-path-to-phase1-training-data>` with the actual path in the datastore that contains the training files.\n",
    "* Update the occurence `<path-to-checkpoint-from-phase-1>` with the path to the final checkpoint from phase 1.\n",
    "\n",
    "To fully utilize capacity, we suggest parameters from below table for phase 2. \n",
    "\n",
    "| VM SKU             | node_count         | global_batch_size         | gpu_feed_batch_size | gradient_accumulation_passes |\n",
    "| ------------------ |:------------------:|-----------------:|-----------------:| ---------------------------:|\n",
    "| Standard_ND40rs_v2 | 1 (8 GPUs total)   | 32,768  | 16 | 256  |\n",
    "| Standard_ND40rs_v2 | 2 (16 GPUs total)  | 32,768  | 16 | 128  |\n",
    "| Standard_ND40rs_v2 | 4 (32 GPUs total)  | 32,768  | 16 | 64   |\n",
    "| Standard_ND40rs_v2 | 8 (64 GPUs total)  | 32,768  | 16  | 32   |\n",
    "| Standard_NC24rs_v3 | 1 (4 GPUs total)   | 32,768  | 8 | 1024 |\n",
    "| Standard_NC24rs_v3 | 2 (8 GPUs total)   | 32,768  | 8 | 512  |\n",
    "| Standard_NC24rs_v3 | 4 (16 GPUs total)  | 32,768  | 8 | 256  |\n",
    "| Standard_NC24rs_v3 | 8 (32 GPUs total)  | 32,768  | 8 | 128  |\n",
    "| Standard_NC24rs_v3 | 16 (64 GPUs total) | 32,768  | 8  | 64   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training estimator for phase 2\n",
    "# Fill in blob path to phase 1 training data as well as phase 1 checkpoint in arguments below\n",
    "estimator_ph2 = Estimator(source_directory=project_folder,\n",
    "\n",
    "                    # Compute configuration\n",
    "                    compute_target = gpu_compute_target,\n",
    "                    node_count=4, \n",
    "                    process_count_per_node=1, # each counts as a separate MPI job\n",
    "                    distributed_training = mpi,\n",
    "                    use_gpu = True,\n",
    "                    \n",
    "                    # Supply Docker image\n",
    "                    use_docker = True,\n",
    "                    custom_docker_image = image_name,\n",
    "                    user_managed = True,\n",
    "\n",
    "                    # Training script parameters\n",
    "                    script_params = {\n",
    "                        '--config_file' : 'bert_config.json',\n",
    "                        '--data_dir' : ds.path('<blob-path-to-phase2-training-data>').as_mount(),\n",
    "                        '--output_dir': ds.path(f'output/{experiment_name}/{output_id}/').as_mount(),\n",
    "                        '--max_seq_length': 512,\n",
    "                        '--max_predictions_per_seq': 80,                        \n",
    "                        '--gpu_feed_batch_size' : 16,\n",
    "                        '--gradient_accumulation_passes' : 64,\n",
    "                        '--allreduce_post_accumulation' : '',                             \n",
    "                        '--fp16' : '',                 \n",
    "                        '--max_steps' : 1563,\n",
    "                        '--init_checkpoint' : ds.path('<path-to-checkpoint-from-phase-1>').as_mount(),                        \n",
    "                        '--num_steps_per_checkpoint' : 200,\n",
    "                        '--num_passes_to_smooth_output' : 32,\n",
    "                        '--learning_rate' : '4e-3',\n",
    "                        '--warmup_proportion' : '0.128',\n",
    "                        '--seed': 42,\n",
    "                    },\n",
    "                    \n",
    "                    entry_script = 'run_pretraining.py',\n",
    "                    inputs = [ds.path('<blob-path-to-phase2-training-data>').as_mount()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run AzureML experiment - Phase 2 of pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit phase 2 run (check logs from Outputs + logs tab of corresponding link)\n",
    "run = experiment.submit(estimator_ph2)\n",
    "print(run.get_portal_url())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
