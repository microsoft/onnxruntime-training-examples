{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate finetuning of GPT2 model for Language Modeling task using ONNX Runtime\n",
    "This notebook contains a walkthrough of using ONNX Runtime in Azure Machine Learning service to finetune [GPT2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) models. This example uses ONNX Runtime to fine-tune the GPT2 PyTorch model maintained at https://github.com/huggingface/transformers.\n",
    "Specificaly, we showcase finetuning the [pretrained GPT2-medium](https://huggingface.co/transformers/pretrained_models.html), which has 345M parameters using both ORT and Pytorch, an compare the performance of both frameworks.\n",
    "\n",
    "Steps:\n",
    "- Intialize an AzureML workspace\n",
    "- Register a datastore to use preprocessed data for training\n",
    "- Create an AzureML experiment\n",
    "- Provision a compute target\n",
    "- Create a PyTorch Estimator\n",
    "- Configure and Run\n",
    "\n",
    "Prerequisites\n",
    "If you are using an Azure Machine Learning [Compute Instance](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-instance) you are all set. Otherwise, you need to setup your environment by installing AzureML Python SDK to run this notebook. Refer to [How to use Estimator in Azure ML](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/how-to-use-estimator/how-to-use-estimator.ipynb) notebook first if you haven't already to establish your connection to the AzureML Workspace. \n",
    "\n",
    "Refer to instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md before running the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check SDK installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# AzureML libraries\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Datastore, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "from azureml.core.runconfig import MpiConfiguration, RunConfiguration, DEFAULT_GPU_IMAGE\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AzureML Workspace setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or retrieve Azure machine learning workspace\n",
    "# see https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py\n",
    "ws = Workspace.get(name=\"myworkspace\", subscription_id='<azure-subscription-id>', resource_group='myresourcegroup')\n",
    "\n",
    "# Print workspace attributes\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Workspace region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Datastore\n",
    "Before running the step below, data prepared using the instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md should be transferred to an Azure Blob container referenced in the `Datastore` registration step. Refer to the documentation at https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data for details on using data in Azure ML experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datastore from blob storage containing training data.\n",
    "# Consult README.md for instructions downloading and uploading training data.\n",
    "ds = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                             datastore_name='<datastore-name>',\n",
    "                                             account_name='<storage-account-name>', \n",
    "                                             account_key='<storage-account-key>',\n",
    "                                             container_name='<storage-container-name>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print datastore attributes\n",
    "print('Datastore name: ' + ds.name, \n",
    "      'Container name: ' + ds.container_name, \n",
    "      'Datastore type: ' + ds.datastore_type, \n",
    "      'Workspace name: ' + ds.workspace.name, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AzureML Compute Cluster\n",
    "This recipe is supported on Azure Machine Learning Service using 16 x Standard_NC24rs_v3 or 8 x Standard_ND40rs_v2 VMs. In the next step, you will create an AzureML Compute cluster of Standard_NC40s_v2 GPU VMs with the specified name, if it doesn't already exist in your workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GPU cluster\n",
    "gpu_cluster_name = \"ortgptfinetune\" \n",
    "try:\n",
    "    gpu_compute_target = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_ND40rs_v2', min_nodes=0, max_nodes=8)\n",
    "    gpu_compute_target = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "    gpu_compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment for phase 1\n",
    "experiment_name = 'gpt2_medium-ort-finetuning'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator\n",
    "Notes before running the following step:\n",
    "* Update the following step to replace two occurences of `<blob-path-to-training-data>` with the actual path in the datastore to the training data.\n",
    "* If you followed instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md to prepare data, make sure that the data and others files that are not code or config are moved out `workspace` directory. Data files should have been moved to a `Datastore` to use in training. \n",
    "* Update the occurance of `<tagged-onnxruntime-gpt-container>` with the tag of the built docker image pushed to a container registry. Similarly, update the `<azure-subscription-id>` and `<container-registry-resource-group>` with the contair registry's subscription ID and resource group.\n",
    "\n",
    "For the Pytorch run, per GPU batch size must be reduced in order to not run out of memory(OOM).\n",
    "\n",
    "| VM SKU             | GPU memory   | gpu_count |    ORT_batch_size    | PyTorch_batch_size|\n",
    "| ------------------ |:----------------:|:---------:|:-------:| -------:|\n",
    "| Standard_ND40rs_v2 | 32 GB            | 8         | 4   | 2 |\n",
    "| Standard_NC24rs_v3 | 16 GB            | 4         | 1   | NA |\n",
    "\n",
    "Pytorch runs OOM even with per_GPU batch size of 1 on a 16GB machine.\n",
    "\n",
    "To get comparable runs between PyTorch and ORT, scale the gradient_accumulation_steps so that the global_batch_size stays the same with changing per_gpu_train_batch_size. Here, the global_batch_size can be calculated as: \n",
    "    `global_batch_size = gradient_accumulation_steps * per_gpu_train_batch_size * node_count * process_count_per_node`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this directory should contain run_language_modeling_ort.py, after files copied over based on the instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md \n",
    "project_folder = '/path/to/onnxruntime-training-examples/huggingface-gpt2/transformers/examples'\n",
    "\n",
    "image_name = '<tagged-onnxruntime-gpt-container>'\n",
    "subscription_id = '<azure-subscription-id>'\n",
    "container_registry_resource_group = '<container-registry-resource-group>'\n",
    "registry_details = None\n",
    "\n",
    "acr = re.match('^((\\w+).azurecr.io)/(.*)', image_name)\n",
    "if acr:\n",
    "    # Extract the relevant parts from the container image\n",
    "    #   e.g. onnxtraining.azurecr.io/onnxruntime-gpt:latest\n",
    "    registry_address = acr.group(1) # onnxtraining.azurecr.io\n",
    "    registry_name = acr.group(2) # onnxtraining\n",
    "    container_image = acr.group(3) # onnxruntime-gpt:latest\n",
    "\n",
    "    registry_client = get_client_from_cli_profile(ContainerRegistryManagementClient, subscription_id=subscription_id)\n",
    "    registry_credentials = registry_client.registries.list_credentials(, registry_name)\n",
    "\n",
    "    registry_details = ContainerRegistry()\n",
    "    registry_details.address = registry_address\n",
    "    registry_details.username = registry_credentials.username\n",
    "    registry_details.password = registry_credentials.passwords[0].value\n",
    "\n",
    "# Define the script parameters.\n",
    "# To run using PyTorch instead of ORT, remove the --ort_trainer flag.\n",
    "script_params = {\n",
    "    \"--model_type\" : \"gpt2-medium\", \n",
    "    \"--model_name_or_path\" : \"gpt2-medium\", \n",
    "    \"--tokenizer_name\" : \"gpt2-medium \", \n",
    "    \"--config_name\" : \"gpt2-medium \", \n",
    "    \"--do_eval\" : \"\", \n",
    "    \"--do_train\": \"\", \n",
    "    \"--train_data_file\" : ds.path('<blob-path-to-training-data>/wiki.train.tokens').as_mount(),\n",
    "    \"--eval_data_file\" : ds.path('<blob-path-to-training-data>/wiki.valid.tokens').as_mount(), \n",
    "    \"--output_dir\" : ds.path(f'output/{experiment_name}/{output_id}/').as_mount(), \n",
    "    \"--per_gpu_train_batch_size\" : \"4\", \n",
    "    \"--per_gpu_eval_batch_size\" : \"4\", \n",
    "    \"--gradient_accumulation_steps\" : \"4\",\n",
    "    \"--block_size\" : \"1024\", \n",
    "    \"--weight_decay\" : \"0.01\", \n",
    "    \"--overwrite_output_dir\" : \"\", \n",
    "    \"--num_train_epocs\" : \"5\",\n",
    "    \"--ort_trainer\" : \"\" # Use ORT to train instead of PyTorch\n",
    "    },\n",
    "\n",
    "# set MPI configuration\n",
    "# set processes per node to be equal to GPU count on SKU.\n",
    "mpi = MpiConfiguration()\n",
    "mpi.process_count_per_node = 8\n",
    "\n",
    "import uuid\n",
    "output_id = uuid.uuid1().hex\n",
    "\n",
    "# Define training estimator for ORT run\n",
    "# Consult https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-ml-models\n",
    "# Fill in blob path to training data in argument below\n",
    "# AzureML Estimator that describes how to run the Experiment\n",
    "estimator_ort = PyTorch(source_directory=project_folder,\n",
    "\n",
    "                    # Compute configuration\n",
    "                    compute_target = gpu_compute_target,\n",
    "                    node_count=4,\n",
    "                    distributed_training = mpi,\n",
    "                    use_gpu = True,\n",
    "                    \n",
    "                    # supply Docker image\n",
    "                    use_docker = True,\n",
    "                    custom_docker_image = image_name,\n",
    "                    image_registry_details=registry_details,\n",
    "                    user_managed = True,\n",
    "                    \n",
    "                    # Training script parameters\n",
    "                    script_params = script_params,\n",
    "                    entry_script = 'run_language_modeling_ort.py',\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run AzureML experiment - Phase 1 of pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit ORT run (check logs from Outputs + logs tab of corresponding link)\n",
    "run = experiment.submit(estimator_ort)\n",
    "RunDetails(run).show()\n",
    "print(run.get_portal_url())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}